{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.measure import moments_hu, shannon_entropy\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.feature import canny, corner_harris, corner_peaks, peak_local_max, daisy, blob_doh, shape_index, hog, hessian_matrix_det, hessian_matrix, structure_tensor\n",
    "from skimage.filters import rank, threshold_otsu, frangi, hessian, roberts, sobel, sobel_h, sobel_v, threshold_sauvola, gabor\n",
    "from skimage.morphology import disk\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import hough_line, hough_line_peaks\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To featurize each image, we first collect the paths to all images contained in each image category using .glob( ).\n",
    "\n",
    "The path in .glob( ) should reflect the location of the image directory '50_categories'.\n",
    "\n",
    "It is not necessary to run this cell, as the featured image data is saved and can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob('/Users/danslaughter/Desktop/AY250/hw6/50_categories/*/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define 3 functions:\n",
    "\n",
    "1. im_x_corr: this function computes the cross correlation between the RGB channels of a given image\n",
    "2. feature_func: this function computes a set of features of a given image\n",
    "3. feature_func_set2: this function computes a second set of different features of a given image\n",
    "\n",
    "The cells containing these 3 functions should be run, as the functions will be necessary to featurize the validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_x_corr(im1, im2, im3):\n",
    "    \n",
    "    #compute the 3 products of the fft of each color channel\n",
    "    products = [np.fft.fft2(im1) * np.fft.fft2(im2).conj(), np.fft.fft2(im1) * np.fft.fft2(im3).conj(), \n",
    "               np.fft.fft2(im2) * np.fft.fft2(im3).conj()]\n",
    "    \n",
    "    #return the shifted ifft of the 3 products\n",
    "    return [np.fft.fftshift(np.fft.ifft2(products[0])), np.fft.fftshift(np.fft.ifft2(products[1])), \n",
    "           np.fft.fftshift(np.fft.ifft2(products[2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_func(img):\n",
    "    \n",
    "    #get image label from image path\n",
    "    file_name = img.split('/')[-1].replace('.jpg', '').replace('_', '')\n",
    "    label = ''.join(x for x in file_name if x.isalpha())\n",
    "    \n",
    "    #read in color image and create grayscale image\n",
    "    im = plt.imread(img)\n",
    "    im_gray = rgb2gray(im)\n",
    "\n",
    "    #create an exception for images that are not RGB\n",
    "    if len(np.shape(im)) != 3:\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        im_R = im[:,:,0]\n",
    "        im_G = im[:,:,1]\n",
    "        im_B = im[:,:,2]\n",
    "                \n",
    "        #feature: mean value of RGB channels and grayscale image\n",
    "        mean_R = np.mean(im_R)\n",
    "        mean_G = np.mean(im_G)\n",
    "        mean_B = np.mean(im_B)\n",
    "        mean_gray = np.mean(im_gray)\n",
    "        \n",
    "        #feature: variance of RGB channels and grayscale image\n",
    "        var_R = np.var(im_R)\n",
    "        var_G = np.var(im_G)\n",
    "        var_B = np.var(im_B)\n",
    "        var_gray = np.var(im_gray)\n",
    "        \n",
    "        #feature: mean and variance of cross correlation between RGB channels\n",
    "        x_corr = im_x_corr(im_R, im_G, im_B)\n",
    "        cc_RG = x_corr[0]\n",
    "        cc_RB = x_corr[1]\n",
    "        cc_GB = x_corr[2]\n",
    "        \n",
    "        xcorr_RG_mean = np.mean(np.abs(cc_RG))\n",
    "        xcorr_RB_mean = np.mean(np.abs(cc_RB))\n",
    "        xcorr_GB_mean = np.mean(np.abs(cc_GB))\n",
    "        \n",
    "        xcorr_RG_var = np.var(np.abs(cc_RG))\n",
    "        xcorr_RB_var = np.var(np.abs(cc_RB))\n",
    "        xcorr_GB_var = np.var(np.abs(cc_GB))\n",
    "        \n",
    "        #feature: structual similarity index between RGB channels\n",
    "        ssim_RG = ssim(im_R, im_G, data_range = im_G.max() - im_G.min())\n",
    "        ssim_RB = ssim(im_R, im_B, data_range = im_B.max() - im_B.min())\n",
    "        ssim_GB = ssim(im_G, im_B, data_range = im_B.max() - im_B.min())\n",
    "        \n",
    "        #feature: number of corners in the image\n",
    "        corner_count = len(corner_peaks(corner_harris(im_gray), min_distance=4))\n",
    "        \n",
    "        #feature: number of extrema in the image\n",
    "        size_filt = int(len(im_gray)/8)\n",
    "        max_filt = ndi.maximum_filter(im_gray, size = size_filt, mode = 'constant')\n",
    "        extrema_count = len(peak_local_max(im_gray, min_distance = size_filt))\n",
    "        \n",
    "        #feature: number of contours at the mean of the image\n",
    "        mean_gray = np.mean(im_gray)\n",
    "        contours_count = len(measure.find_contours(im_gray, mean_gray))\n",
    "        \n",
    "        #feature: mean and variance of entropy of the image\n",
    "        disk_size = int(len(im_gray)/8)\n",
    "        shift = (im_gray).astype(np.uint8)\n",
    "        im_entropy = rank.entropy(shift, disk(disk_size))\n",
    "        im_entropy_mean = np.mean(im_entropy)\n",
    "        im_entropy_var = np.var(im_entropy)\n",
    "        \n",
    "        #feature: shannon entropy of the image\n",
    "        shannonEntropy = shannon_entropy(im_gray)\n",
    "        \n",
    "        #feature: mean and variance of thresholded image\n",
    "        thresh = threshold_otsu(im_gray)\n",
    "        binary = im_gray > thresh\n",
    "        binary_mean = np.mean(binary)\n",
    "        binary_var = np.var(binary)\n",
    "        \n",
    "        #feature: mean and variance of frangi filtered image\n",
    "        im_frangi = frangi(im_gray)\n",
    "        frangi_mean = np.mean(im_frangi)\n",
    "        frangi_var = np.var(im_frangi)\n",
    "        \n",
    "        #feature: mean and variance of hessian filtered image\n",
    "        im_hessian = hessian(im_gray)\n",
    "        hessian_mean = np.mean(im_hessian)\n",
    "        hessian_var = np.var(im_hessian)\n",
    "        \n",
    "        #feature: mean and variance of canny edge filtered image\n",
    "        edge_canny = canny(im_gray, sigma=3)\n",
    "        edge_canny_mean = np.mean(edge_canny)\n",
    "        edge_canny_var = np.var(edge_canny)\n",
    "        \n",
    "        #feature: number of \"blobs\" in the image\n",
    "        blobs_count = len(blob_doh(im_gray, max_sigma=30, threshold=.01)[:,0])\n",
    "        \n",
    "        #feature: number of straight lines in the image\n",
    "        h, theta, d = hough_line(im_gray)\n",
    "        peaks = hough_line_peaks(h, theta, d)\n",
    "        peak_count = np.shape(peaks)[1]\n",
    "        \n",
    "        #feature: mean and variance of sobel edge filtered image\n",
    "        edge_sobel = sobel(im_gray)\n",
    "        edge_sobel_mean = np.mean(edge_sobel)\n",
    "        edge_sobel_var = np.var(edge_sobel)\n",
    "        \n",
    "        #feature: mean and variance of vertical/horizontal sobel edge filtered image\n",
    "        edge_sobel_h = sobel_h(im_gray)\n",
    "        edge_sobel_v = sobel_v(im_gray)\n",
    "        \n",
    "        edge_sobel_h_mean = np.mean(edge_sobel_h)\n",
    "        edge_sobel_v_mean = np.mean(edge_sobel_v)\n",
    "        \n",
    "        edge_sobel_h_var = np.var(edge_sobel_h)\n",
    "        edge_sobel_v_var = np.var(edge_sobel_v)\n",
    "        \n",
    "        #feature: number of DAISY descriptors of the image\n",
    "        descs = daisy(im_gray, step = int(len(im_gray)/20), radius = int(len(im_gray)/8), rings = 3, \n",
    "                             histograms = 8, orientations = 8)\n",
    "        descs_num = descs.shape[0] * descs.shape[1]\n",
    "\n",
    "        #feature: mean and variance of 6 gabor filters applied to image \n",
    "        #(2 freqencies (0.1, 0.5), 3 theta orientations (0, 45, 90)) \n",
    "        #this computation is particularly slow, so it has been parallelized using concurrent.futures.\n",
    "        iters_im = [im_gray, im_gray, im_gray]\n",
    "        iters_low_freq = [0.1, 0.1, 0.1]\n",
    "        iters_high_freq = [0.5, 0.5, 0.5]\n",
    "        iters_theta = [0, np.pi / 4, np.pi / 2]\n",
    "        \n",
    "        executor = ProcessPoolExecutor(4) #set up a pool of 4 workers processes\n",
    "        filt_R_C_low = list(executor.map(gabor, iters_im, iters_low_freq, iters_theta))\n",
    "        filt_R_C_high = list(executor.map(gabor, iters_im, iters_high_freq, iters_theta))\n",
    "        executor.shutdown() #close down pool\n",
    "        \n",
    "        filt_real_1_0_mean = np.mean(filt_R_C_low[0][0])\n",
    "        filt_real_1_1_mean = np.mean(filt_R_C_low[1][0])\n",
    "        filt_real_1_2_mean = np.mean(filt_R_C_low[2][0])\n",
    "        \n",
    "        filt_real_1_0_var = np.var(filt_R_C_low[0][0])\n",
    "        filt_real_1_1_var = np.var(filt_R_C_low[1][0])\n",
    "        filt_real_1_2_var = np.var(filt_R_C_low[2][0])\n",
    "        \n",
    "        filt_real_5_0_mean = np.mean(filt_R_C_high[0][0])\n",
    "        filt_real_5_1_mean = np.mean(filt_R_C_high[1][0])\n",
    "        filt_real_5_2_mean = np.mean(filt_R_C_high[2][0])\n",
    "        \n",
    "        filt_real_5_0_var = np.var(filt_R_C_high[0][0])\n",
    "        filt_real_5_1_var = np.var(filt_R_C_high[1][0])\n",
    "        filt_real_5_2_var = np.var(filt_R_C_high[2][0])\n",
    "                \n",
    "        return [mean_R, mean_G, mean_B, mean_gray, var_R, var_G, var_B, var_gray, xcorr_RG_mean, xcorr_RB_mean, \n",
    "                xcorr_GB_mean, xcorr_RG_var, xcorr_RB_var, xcorr_GB_var, ssim_RG, ssim_RB, ssim_GB, corner_count, \n",
    "                extrema_count, contours_count, im_entropy_mean, im_entropy_var, shannonEntropy, binary_mean, \n",
    "                binary_var, frangi_mean, frangi_var, hessian_mean, hessian_var, edge_canny_mean, edge_canny_var, \n",
    "                blobs_count, peak_count, edge_sobel_mean, edge_sobel_var, edge_sobel_h_mean, edge_sobel_v_mean, \n",
    "                edge_sobel_h_var, edge_sobel_v_var, descs_num, filt_real_1_0_mean, filt_real_1_1_mean, filt_real_1_2_mean, \n",
    "                filt_real_1_0_var, filt_real_1_1_var, filt_real_1_2_var, filt_real_5_0_mean, filt_real_5_1_mean, \n",
    "                filt_real_5_2_mean, filt_real_5_0_var, filt_real_5_1_var, filt_real_5_2_var, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_func_set2(img):\n",
    "    \n",
    "    #get image label from image path\n",
    "    file_name = img.split('/')[-1].replace('.jpg', '').replace('_', '')\n",
    "    label = ''.join(x for x in file_name if x.isalpha())\n",
    "    \n",
    "    #read in color image and create grayscale image\n",
    "    im = plt.imread(img)\n",
    "    im_gray = rgb2gray(im)\n",
    "\n",
    "    #create an exception for images that are not RGB\n",
    "    if len(np.shape(im)) != 3:\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "\n",
    "        #feature: mean and variance of histogram of oriented gradients\n",
    "        hist_grad = hog(im_gray, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(1, 1), \n",
    "                        block_norm = 'L2-Hys')\n",
    "        hist_grad_mean = np.mean(hist_grad)\n",
    "        hist_grad_var = np.var(hist_grad)\n",
    "        \n",
    "        #feature: mean and variance of the image DAISY descriptors\n",
    "        descs = daisy(im_gray, step = int(len(im_gray)/20), radius = int(len(im_gray)/8), rings = 3, \n",
    "                      histograms = 8, orientations = 8)\n",
    "        descs_mean = np.mean(descs)\n",
    "        descs_var = np.var(descs)\n",
    "        \n",
    "        #features: mean and variance of the image hessian matrix determinant\n",
    "        hess_det = hessian_matrix_det(im_gray, sigma=1)\n",
    "        hess_det_mean = np.mean(hess_det)\n",
    "        hess_det_var = np.var(hess_det)\n",
    "        \n",
    "        #feature: mean and variance of the 3 image structure tensor elements\n",
    "        A00, A01, A11 = structure_tensor(im_gray, sigma=1, mode='constant', cval=0)\n",
    "        A00_mean = np.mean(A00)\n",
    "        A00_var = np.var(A00)\n",
    "        A01_mean = np.mean(A01)\n",
    "        A01_var = np.var(A01)\n",
    "        A11_mean = np.mean(A11)\n",
    "        A11_var = np.var(A11)\n",
    "        \n",
    "        #feature: mean and variance of the 3 image hessian matrix elements\n",
    "        H00, H01, H11 = hessian_matrix(im_gray, sigma=1, mode='constant', cval=0, order='rc')\n",
    "        H00_mean = np.mean(H00)\n",
    "        H00_var = np.var(H00)\n",
    "        H01_mean = np.mean(H01)\n",
    "        H01_var = np.var(H01)\n",
    "        H11_mean = np.mean(H11)\n",
    "        H11_var = np.var(H11)\n",
    "        \n",
    "        #feature: the 6 hu moments of the image\n",
    "        hu0, hu1, hu2, hu3, hu4, hu5, hu6 = moments_hu(im_gray)\n",
    "                \n",
    "        return [hist_grad_mean, hist_grad_var, descs_mean, descs_var, hess_det_mean, hess_det_var, A00_mean, A00_var, \n",
    "               A01_mean, A01_var, A11_mean, A11_var, H00_mean, H00_var, H01_mean, H01_var, H11_mean, H11_var, \n",
    "               hu0, hu1, hu2, hu3, hu4, hu5, hu6, label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an empty pandas dataframe with columns named after the first set of features computed in feature_func( ), and another empty pandas dataframe with columns named after the second set of features computed in feature_func_set2( ). These dataframes are then populated with the features of each image by calling the two featurizing functions on each image path we collected earlier using .glob( ).\n",
    "\n",
    "This cell does not need to be run, as the featurized data has been saved and can be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######### Note #########\n",
    "# this cell takes ~8 hours to run and does not need to be run again\n",
    "# the image features are saved and can be loaded in\n",
    "\n",
    "#create two lists with the names of the image features contained in the two image feature sets\n",
    "cols = ['mean_R', 'mean_G', 'mean_B', 'mean_gray', 'var_R', 'var_G', 'var_B', 'var_gray', 'xcorr_RG_mean', 'xcorr_RB_mean', \n",
    "        'xcorr_GB_mean', 'xcorr_RG_var', 'xcorr_RB_var', 'xcorr_GB_var', 'ssim_RG', 'ssim_RB', 'ssim_GB', 'corner_count', \n",
    "        'extrema_count', 'contours_count', 'im_entropy_mean', 'im_entropy_var', 'shannonEntropy', 'binary_mean', \n",
    "        'binary_var', 'frangi_mean', 'frangi_var', 'hessian_mean', 'hessian_var', 'edge_canny_mean', 'edge_canny_var', \n",
    "        'blobs_count', 'peak_count', 'edge_sobel_mean', 'edge_sobel_var', 'edge_sobel_h_mean', 'edge_sobel_v_mean', \n",
    "        'edge_sobel_h_var', 'edge_sobel_v_var', 'descs_num', 'filt_real_1_0_mean', 'filt_real_1_1_mean', 'filt_real_1_2_mean', \n",
    "        'filt_real_1_0_var', 'filt_real_1_1_var', 'filt_real_1_2_var', 'filt_real_5_0_mean', 'filt_real_5_1_mean', \n",
    "        'filt_real_5_2_mean', 'filt_real_5_0_var', 'filt_real_5_1_var', 'filt_real_5_2_var', 'label']\n",
    "\n",
    "cols_set2 = ['hist_grad_mean', 'hist_grad_var', 'descs_mean', 'descs_var', 'hess_det_mean', 'hess_det_var', 'A00_mean', 'A00_var', \n",
    "        'A01_mean', 'A01_var', 'A11_mean', 'A11_var', 'H00_mean', 'H00_var', 'H01_mean', 'H01_var', 'H11_mean', 'H11_var', \n",
    "        'hu0', 'hu1', 'hu2', 'hu3', 'hu4', 'hu5', 'hu6', 'label']\n",
    "\n",
    "#create two empty pandas dataframes that will contain the 2 sets of features of \n",
    "#each image with columns named using cols and cols_set2\n",
    "im_feature_DF = pd.DataFrame(data = None, columns = cols)\n",
    "im_feature_DF_set2 = pd.DataFrame(data = None, columns = cols_set2)\n",
    "\n",
    "#populate the pandas dataframes by calling the featurizing functions on each image path\n",
    "for index, path in enumerate(paths):\n",
    "    feature_row_DF = pd.DataFrame(feature_func(path))\n",
    "    feature_row_DF_set2 = pd.DataFrame(feature_func_set2(path))\n",
    "    if len(feature_row_DF) == len(cols):\n",
    "        im_feature_DF.loc[index] = (np.transpose(feature_row_DF.values)[0])\n",
    "    if len(feature_row_DF_set2) == len(cols_set2):\n",
    "        im_feature_DF_set2.loc[index] = (np.transpose(feature_row_DF_set2.values)[0])\n",
    "        \n",
    "#save the pandas dataframes as .csv files, so they can be loaded later\n",
    "im_feature_DF.to_csv('/Users/danslaughter/Desktop/image_features_set_1_.csv')\n",
    "im_feature_DF_set2.to_csv('/Users/danslaughter/Desktop/image_features_set_2_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load in the two data sets containing the featurized images using .read_csv( ). These dataframes are then reformatted and concatenated to form a single dataframe containing all 72 features. In order to assign which images will be used for training and which for testing, we can simply add a column to the dataframe with a boolean value to distinguish between training and testing. This boolean is assigned by sampling a number from a uniform distribution between 0 and 1 and checking if the number is less than or equal to 0.5. This randomly assigns a ~50/50 split to the featurized images. The testing and training data is then further split into the image feature data and the image category label.\n",
    "\n",
    "This cell does not need to be run, as an optimized random forest classifier has been pickled and can be loaded in to use on the validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 2121\n",
      "testing size: 2089\n"
     ]
    }
   ],
   "source": [
    "#read in the .csv file containing the first set of features of all the images to a dataframe and reformat it\n",
    "#the path in .read_csv() should reflect the location of the first image feature dataset on the machine\n",
    "im_feature_DF_set1 = pd.read_csv('/Users/danslaughter/Desktop/image_features_set1.csv')\n",
    "im_feature_DF_set1 = im_feature_DF_set1.drop(['Unnamed: 0'], axis = 1)\n",
    "im_feature_DF_set1 = im_feature_DF_set1.drop(['label'], axis = 1)\n",
    "\n",
    "#read in the .csv file containing the second set of features of all the images to a dataframe and reformat it\n",
    "#the path in .read_csv() should reflect the location of the second image feature dataset on the machine\n",
    "im_feature_DF_set2 = pd.read_csv('/Users/danslaughter/Desktop/image_features_set_2.csv')\n",
    "im_feature_DF_set2 = im_feature_DF_set2.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "#concatenate the two dataframes to make a single dataframe\n",
    "im_feature_DF = pd.concat([im_feature_DF_set1, im_feature_DF_set2], axis = 1)\n",
    "\n",
    "#assign a boolean value to a new column called 'is_train'\n",
    "#boolean value generated by sampling a uniform distribution and checking if the value is <= 0.5\n",
    "im_feature_DF['is_train'] = np.random.uniform(0, 1, len(im_feature_DF)) <= 0.5\n",
    "\n",
    "#split dataframe into testing and training data\n",
    "im_train, im_test = im_feature_DF[im_feature_DF['is_train'] == True], im_feature_DF[im_feature_DF['is_train'] == False]\n",
    "\n",
    "#split training data into raw data and truth value (image category)\n",
    "X_im_train = im_train.drop(['is_train'], axis = 1)\n",
    "X_im_train = X_im_train.drop(['label'], axis = 1)\n",
    "Y_im_train = im_train['label']\n",
    "print(\"training size: \" + str(len(Y_im_train)))\n",
    "\n",
    "#split testing data into raw data and truth value (image category)\n",
    "X_im_test = im_test.drop(['is_train'], axis = 1)\n",
    "X_im_test = X_im_test.drop(['label'], axis = 1)\n",
    "Y_im_test = im_test['label']\n",
    "print(\"testing size: \" + str(len(Y_im_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we execute a .GridSearchCV( ) using 5-fold cross validation to locate an optimum random forest classifier. We vary 4 parameters: the number of estimators (number of trees in the forest), the maximum number of features to be considered when finding the optimum split, the criteria used to determine the optimum split, and the minumum number of samples required to split a node. Once the best model is found, an estimator is retuned using the optimum parameters on the whole dataset, which then is used to fit the training data. Some metrics on the model's cross validated performance are also displayed. This optimum random forest classifier is then pickled, so that it can be loaded later for use.\n",
    "\n",
    "This cell does not need to be run, since a random forest classifier is saved and can be loaded in to use on the validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best zero-one score: 0.314474304573\n",
      "\n",
      "Optimal Model:\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=12, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Cross Validation Results - Mean Test Scores:\n",
      "[ 0.29891561  0.30221594  0.30221594  0.30174446  0.29891561  0.30740217\n",
      "  0.30504479  0.29561528  0.3107025   0.30221594  0.30410184  0.3107025\n",
      "  0.30551627  0.3088166   0.31164545  0.3088166   0.30834512  0.30645922\n",
      "  0.30551627  0.3125884   0.3144743   0.30740217  0.30787364  0.30504479\n",
      "  0.31164545  0.31164545  0.30787364  0.29184347  0.29090052  0.29608675\n",
      "  0.2932579   0.30457331  0.3107025   0.29844413  0.30410184  0.31305988\n",
      "  0.29608675  0.2951438   0.28524281  0.29420085  0.30975955  0.2970297\n",
      "  0.29985856  0.29938708  0.31164545  0.30315889  0.30080151  0.30693069\n",
      "  0.30221594  0.30410184  0.30645922  0.30363036  0.30363036  0.30834512]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "#explore 3 different forest sizes, 3 choices of max features to consider on a split, \n",
    "#2 different split criteria, and 3 min sample splits\n",
    "parameters = {'n_estimators': [100, 125, 150],  'max_features': ['auto', 10, 12], \n",
    "             'criterion': ['gini', 'entropy'], 'min_samples_split': [2, 3, 4]}\n",
    "rf_tune = model_selection.GridSearchCV(RandomForestClassifier(), parameters, n_jobs = -1, cv = 5, verbose = 1)\n",
    "\n",
    "#fit the optimum classifier identified by .GridSearchCV() to the training data\n",
    "rf_opt = rf_tune.fit(X_im_train, Y_im_train)\n",
    "\n",
    "print(\"Best zero-one score: \" + str(rf_opt.best_score_) + \"\\n\")\n",
    "print(\"Optimal Model:\\n\" + str(rf_opt.best_estimator_) + \"\\n\")\n",
    "print(\"Cross Validation Results - Mean Test Scores:\\n\" + str(rf_opt.cv_results_['mean_test_score']))\n",
    "\n",
    "#pickle the optimum random forest classifier\n",
    "with open('/Users/danslaughter/Desktop/random_forest_opt.pickle', 'wb') as f:\n",
    "    pickle.dump(rf_opt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the classifier is deployed on the test data and scored based on its zero-one loss and accuracy. The confusion matrix (a small portion of it) is also displayed. This random forest classifier seems to correctly classify test images ~31% of the time. Randomly guessing would have a sucess rate of 2%, so the random forest is ~15 times better than randomly guessing, but performs poorly compared to scores that could be expected from a convolutional neural network.\n",
    "\n",
    "This cell does not need to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-One Loss: 0.687888942078\n",
      "Zero-One Score: 0.312111057922\n",
      "Confusion Matrix:\n",
      "[i, j] is the # of objects truly in group i but predicted to be in group j\n",
      "[[251   0   0 ...,   0   0   0]\n",
      " [  0   5   0 ...,   1   0   1]\n",
      " [  2   0   0 ...,   1   1   0]\n",
      " ..., \n",
      " [  1   0   0 ...,   6   0   0]\n",
      " [  2   1   0 ...,   1   7   0]\n",
      " [  0   0   0 ...,   1   0  17]]\n"
     ]
    }
   ],
   "source": [
    "#deploy the random forest on the test data\n",
    "pred_rf = rf_opt.predict(X_im_test)\n",
    "\n",
    "#compute the zero-one loss, accuracy, and confusion matrix\n",
    "rf_01 = metrics.zero_one_loss(Y_im_test, pred_rf) # zero-one loss\n",
    "rf_01_score = metrics.accuracy_score(Y_im_test, pred_rf) # zero-one score\n",
    "rf_confmat = metrics.confusion_matrix(Y_im_test, pred_rf) # conf mat\n",
    "\n",
    "print(\"Zero-One Loss: \" + str(rf_01))\n",
    "print(\"Zero-One Score: \" + str(rf_01_score))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"[i, j] is the # of objects truly in group i but predicted to be in group j\")\n",
    "print(rf_confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we display the image features sorted by their importances. The number of DAISY descriptors is the most important predictor in classifying an image, followed by the shannon entropy of the image and the mean of the H00 Hessian matrix element of the image.\n",
    "\n",
    "This cell does not need to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.058580500682587126, 'descs_num'),\n",
       " (0.021384766978738375, 'shannonEntropy'),\n",
       " (0.01722454761311901, 'H00_mean'),\n",
       " (0.016638203755440555, 'hu3'),\n",
       " (0.016298386238601924, 'hessian_var'),\n",
       " (0.016102559968904317, 'hu4'),\n",
       " (0.016081986999648416, 'peak_count'),\n",
       " (0.016060960247314567, 'H11_mean'),\n",
       " (0.0158547160023868, 'hessian_mean'),\n",
       " (0.015623469597169046, 'ssim_RG'),\n",
       " (0.015511549275647069, 'hu5'),\n",
       " (0.01549280976654221, 'edge_sobel_h_mean'),\n",
       " (0.015257156522049004, 'ssim_GB'),\n",
       " (0.015192764000698519, 'descs_var'),\n",
       " (0.015147775347590162, 'ssim_RB'),\n",
       " (0.015044313431258012, 'filt_real_1_2_var'),\n",
       " (0.014877348791670176, 'contours_count'),\n",
       " (0.014801275303393334, 'filt_real_1_0_var'),\n",
       " (0.01441442969406545, 'hu0'),\n",
       " (0.014384054609466188, 'hu1'),\n",
       " (0.01424748126001244, 'edge_sobel_v_mean'),\n",
       " (0.014115114466849281, 'filt_real_1_1_var'),\n",
       " (0.014046178189347813, 'A01_mean'),\n",
       " (0.014023482866490571, 'mean_B'),\n",
       " (0.013937640608535971, 'extrema_count'),\n",
       " (0.013928880415925094, 'hist_grad_var'),\n",
       " (0.013861271370159, 'var_R'),\n",
       " (0.013698040876480407, 'xcorr_RG_var'),\n",
       " (0.01328809147400169, 'binary_var'),\n",
       " (0.01327296897104541, 'edge_canny_mean'),\n",
       " (0.013230033336647686, 'hu2'),\n",
       " (0.013224246085761737, 'A01_var'),\n",
       " (0.013137344009817467, 'xcorr_RB_var'),\n",
       " (0.012947911137329043, 'hess_det_mean'),\n",
       " (0.012922374990157992, 'var_gray'),\n",
       " (0.012860131498817, 'binary_mean'),\n",
       " (0.012836491395808352, 'hist_grad_mean'),\n",
       " (0.01282793683920648, 'xcorr_GB_var'),\n",
       " (0.012657491171900785, 'var_G'),\n",
       " (0.012646495255231508, 'var_B'),\n",
       " (0.012604552193436869, 'edge_canny_var'),\n",
       " (0.012548074820446483, 'xcorr_GB_mean'),\n",
       " (0.01242501510570071, 'filt_real_5_0_var'),\n",
       " (0.012371519603812201, 'xcorr_RB_mean'),\n",
       " (0.012357984941174256, 'filt_real_5_2_var'),\n",
       " (0.012208833101352966, 'mean_R'),\n",
       " (0.012150634945941305, 'edge_sobel_v_var'),\n",
       " (0.011981982738665088, 'hu6'),\n",
       " (0.011889001764591229, 'im_entropy_var'),\n",
       " (0.011827599095167311, 'filt_real_5_1_var'),\n",
       " (0.011789240796407006, 'frangi_mean'),\n",
       " (0.011778814734181211, 'H01_var'),\n",
       " (0.011588799094676833, 'corner_count'),\n",
       " (0.011581578817905105, 'im_entropy_mean'),\n",
       " (0.011536992622074293, 'A00_var'),\n",
       " (0.011458838353496892, 'edge_sobel_mean'),\n",
       " (0.01137962830080586, 'H00_var'),\n",
       " (0.011323854811410815, 'xcorr_RG_mean'),\n",
       " (0.01127147041485305, 'A11_var'),\n",
       " (0.011084789134029299, 'A00_mean'),\n",
       " (0.010881592000541748, 'H11_var'),\n",
       " (0.010730791699624027, 'edge_sobel_h_var'),\n",
       " (0.01062134665034292, 'blobs_count'),\n",
       " (0.01038127616044047, 'A11_mean'),\n",
       " (0.0098849149603144598, 'mean_G'),\n",
       " (0.0096862044710796392, 'edge_sobel_var'),\n",
       " (0.0093283386593238163, 'filt_real_5_1_mean'),\n",
       " (0.0090111514893484266, 'mean_gray'),\n",
       " (0.0087299769667949934, 'filt_real_5_0_mean'),\n",
       " (0.0086681110005416319, 'filt_real_1_1_mean'),\n",
       " (0.0085907960217350109, 'filt_real_5_2_mean'),\n",
       " (0.0085047371549531996, 'H01_mean'),\n",
       " (0.0083782056984621489, 'filt_real_1_2_mean'),\n",
       " (0.0081362464296738744, 'filt_real_1_0_mean'),\n",
       " (0.0036239242008788415, 'hess_det_var'),\n",
       " (0.0, 'frangi_var'),\n",
       " (0.0, 'descs_mean')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(zip(rf_opt.best_estimator_.feature_importances_, list(X_im_train)), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we define a function, run_final_classifier( ), to be called on the validation image directory. This function takes as its argument the path to the directory containing the validation images. The path should end with the character '/', i.e. '/Users/me/Desktop/validation/'. The images contained in the directory are then featurized using the two featurizing functions defined earlier. Pandas dataframes are then populated with the resulting data. The optimum random forest classifier that was pickled is then loaded in and deployed on the featurized validation images. The predictions, along with the filenames, are then saved to a .txt file. \n",
    "\n",
    "This cell should be run after two changes are made. There are two paths that need to be edited to reflect the locations of the files on your machine: \n",
    "1. the location of the pickled random forest classifier\n",
    "2. the location to save the output .txt file containing the predicted labels and image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_final_classifier(dir_path):\n",
    "    \n",
    "    #collect the paths to all images contained in the validation directory\n",
    "    final_test_paths = sorted(glob.glob(dir_path + '*'))\n",
    "    \n",
    "    #contruct a list of the image file names\n",
    "    filenames = []\n",
    "    for filename in final_test_paths:\n",
    "        filenames.append(filename.split('/')[-1])\n",
    "     \n",
    "    #create two lists with the names of the image features contained in the two image feature sets\n",
    "    cols_set1 = ['mean_R', 'mean_G', 'mean_B', 'mean_gray', 'var_R', 'var_G', 'var_B', 'var_gray', 'xcorr_RG_mean', \n",
    "                 'xcorr_RB_mean', 'xcorr_GB_mean', 'xcorr_RG_var', 'xcorr_RB_var', 'xcorr_GB_var', 'ssim_RG', \n",
    "                 'ssim_RB', 'ssim_GB', 'corner_count', 'extrema_count', 'contours_count', 'im_entropy_mean', \n",
    "                 'im_entropy_var', 'shannonEntropy', 'binary_mean', 'binary_var', 'frangi_mean', 'frangi_var', \n",
    "                 'hessian_mean', 'hessian_var', 'edge_canny_mean', 'edge_canny_var', 'blobs_count', 'peak_count', \n",
    "                 'edge_sobel_mean', 'edge_sobel_var', 'edge_sobel_h_mean', 'edge_sobel_v_mean', 'edge_sobel_h_var', \n",
    "                 'edge_sobel_v_var', 'descs_num', 'filt_real_1_0_mean', 'filt_real_1_1_mean', 'filt_real_1_2_mean', \n",
    "                 'filt_real_1_0_var', 'filt_real_1_1_var', 'filt_real_1_2_var', 'filt_real_5_0_mean', \n",
    "                 'filt_real_5_1_mean', 'filt_real_5_2_mean', 'filt_real_5_0_var', 'filt_real_5_1_var', \n",
    "                 'filt_real_5_2_var', 'label']\n",
    "    \n",
    "    cols_set2 = ['hist_grad_mean', 'hist_grad_var', 'descs_mean', 'descs_var', 'hess_det_mean', 'hess_det_var', \n",
    "                 'A00_mean', 'A00_var', 'A01_mean', 'A01_var', 'A11_mean', 'A11_var', 'H00_mean', 'H00_var', \n",
    "                 'H01_mean', 'H01_var', 'H11_mean', 'H11_var', 'hu0', 'hu1', 'hu2', 'hu3', 'hu4', 'hu5', 'hu6', \n",
    "                 'label']\n",
    "    \n",
    "    #create two empty pandas dataframes to contain the 2 sets of features of \n",
    "    #each image with columns named using cols_set1 and cols_set2\n",
    "    im_feature_DF_set1 = pd.DataFrame(data = None, columns = cols_set1)\n",
    "    im_feature_DF_set2 = pd.DataFrame(data = None, columns = cols_set2)\n",
    "    \n",
    "    #populate the pandas dataframes by calling the featurizing functions on each image path\n",
    "    for index, path in enumerate(final_test_paths):\n",
    "        feature_row_DF_set1 = pd.DataFrame(feature_func(path))\n",
    "        feature_row_DF_set2 = pd.DataFrame(feature_func_set2(path))\n",
    "        if len(feature_row_DF_set1) == len(cols_set1):\n",
    "            im_feature_DF_set1.loc[index] = (np.transpose(feature_row_DF_set1.values)[0])\n",
    "        if len(feature_row_DF_set2) == len(cols_set2):\n",
    "            im_feature_DF_set2.loc[index] = (np.transpose(feature_row_DF_set2.values)[0])    \n",
    "    \n",
    "    im_feature_DF_set1 = im_feature_DF_set1.drop(['label'], axis = 1)\n",
    "\n",
    "    #concatenate the two dataframes to make a single dataframe\n",
    "    im_feature_DF = pd.concat([im_feature_DF_set1, im_feature_DF_set2], axis = 1)\n",
    "        \n",
    "    X_validation_im_test = im_feature_DF.drop('label', axis = 1)\n",
    "    \n",
    "    #load in pickled random forest classifier\n",
    "    ############## edit path below ##############\n",
    "    #change path in .load() to reflect location of file random_forest_opt.pickle\n",
    "    with open('/Users/danslaughter/Desktop/random_forest_opt.pickle', 'rb') as f:\n",
    "        random_forest_opt = pickle.load(f)\n",
    "    \n",
    "    #deploy the random forest on the featurized validation image data\n",
    "    predict_random_forest = random_forest_opt.predict(X_validation_im_test)\n",
    "    \n",
    "    #create a list containing each predicted class and the corresponding file name\n",
    "    predictions = list(zip(predict_random_forest, filenames))\n",
    "    \n",
    "    formatted_predictions = [('filename', 'predicted_class')] + [('-'*25,'')] + predictions\n",
    "    \n",
    "    #save predicted labels and corresponding image file names as a .txt file\n",
    "    ############## edit path below ##############\n",
    "    #change path in .savetxt() to reflect location to save validation predictions file\n",
    "    np.savetxt('/Users/danslaughter/Desktop/validation_prediction.txt', formatted_predictions, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the final classifier can be called on the directory containg the validation images. The path in the function run_final_classifier( ) should reflect the location of the validation image directory on your machine. This cell may take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#change path in run_final_classifier() to reflect location of the validation image directory on your machine\n",
    "run_final_classifier('/Users/danslaughter/Desktop/validation/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
